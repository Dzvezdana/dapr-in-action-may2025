{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoXVSenoEcMN"
   },
   "source": [
    "# Dapr in Action: From Core Concepts to AI Agents\n",
    "\n",
    "Welcome to the \"Dapr in Action: From Core Concepts to AI Agents\" workshop!\n",
    "\n",
    "This workshop provides a hands-on introduction to Dapr Agents through simple examples. By the end of this workshop, you will be able to:\n",
    "\n",
    "* Create and interact with LLMs using Dapr Agents\n",
    "* Build custom tools for agents to use\n",
    "* Implement the ReAct pattern (Reasoning + Action)\n",
    "* Create stateful workflows with multiple LLM steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYsgOsfWRnur"
   },
   "source": [
    "### Notebook 1. Basic LLM Usage & Understanding Response Objects\n",
    "\n",
    "This notebook demonstrates how to use the `HFHubChatClient` in [dapr-agents](https://github.com/dapr/dapr-agents/tree/main) for basic tasks. We will explore how to:\n",
    "\n",
    "* Initialize the Hugging Face Chat client.\n",
    "* Generate responses to simple prompts.\n",
    "* Use a `.prompty` file to provide context/history for enhanced generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtiywbjXOkwz"
   },
   "source": [
    "#### **[Only if you are using Google Colab]** Setting up dependencies\n",
    "\n",
    "First, let's install all the dependencies. Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOK2gzazRjzZ"
   },
   "outputs": [],
   "source": [
    "!pip install dapr-agents==0.5.1\n",
    "!pip install yfinance>=0.2.61\n",
    "!pip install prompty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTCAD-Zoj6Ux"
   },
   "source": [
    "#### **[Only if you are using Google Colab]** Setting up secrets\n",
    "\n",
    "Next, we need to set the secrets. In Google Colab, in the left-side menu navigate to Secrets->Add new secret.\n",
    "\n",
    "Create two secrets with Notebook access called: `HUGGINGFACE_API_KEY` and `CURRENTS_API_KEY`.\n",
    "\n",
    "You can find their values [here](https://privatebin.net/?ec00cd7f2c464e55#8xMHSFNHyTrGe8f1DhyB62qCmb9RwrPoFExV5pWKXZEs). The password will be shared during the workshop. You can also create your own free keys here (it requires creating an account):\n",
    "\n",
    "- https://currentsapi.services/en\n",
    "- https://huggingface.co/\n",
    "\n",
    "Then execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMmjpIjK6a1q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = userdata.get('HUGGINGFACE_API_KEY')\n",
    "os.environ[\"CURRENTS_API_KEY\"] = userdata.get('CURRENTS_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXT8U0ah6oAo"
   },
   "source": [
    "#### **[Only if you are using Jupyter Notebooks]** Setting up secrets\n",
    "\n",
    "Next, we need to set the secrets. You can find their values [here](https://privatebin.net/?ec00cd7f2c464e55#8xMHSFNHyTrGe8f1DhyB62qCmb9RwrPoFExV5pWKXZEs). The password will be shared during the workshop. You can also create your own free keys here (it requires creating an account):\n",
    "\n",
    "- https://currentsapi.services/en\n",
    "- https://huggingface.co/\n",
    "\n",
    "Set the secrets values in `helper_secrets.py`. Then execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AqAKbjWx69GW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import helper_secrets\n",
    "\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = helper_secrets.HUGGINGFACE_API_KEY\n",
    "os.environ[\"CURRENTS_API_KEY\"] = helper_secrets.CURRENTS_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHifw0hiUqf2"
   },
   "source": [
    "#### Exercise 1 (a): Setup and call the HuggingFace Client using Dapr Agents\n",
    "\n",
    "Run the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0S3IdAw_RX6j"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danaarsovska/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dapr_agents import HFHubChatClient\n",
    "from dapr_agents.types import UserMessage\n",
    "from prompty import Prompty\n",
    "\n",
    "llm = HFHubChatClient(\n",
    "    api_key=os.getenv(\"HUGGINGFACE_API_KEY\"),\n",
    "    model='microsoft/Phi-3-mini-4k-instruct'\n",
    "    )\n",
    "\n",
    "response = llm.generate(\n",
    "    messages=[UserMessage(\"What is the impact of AI on the news? Limit the output to 1 sentance.\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pPjGh_tQBPX"
   },
   "source": [
    "**Expected output:** The LLM will respond with an explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdjFJnLOQB4n"
   },
   "source": [
    "Next, let's analyze the response object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJ_lc7JfQHMK",
    "outputId": "97217afd-44fc-4875-9112-d58c10edae92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response object: choices=[Choice(finish_reason='stop', index=0, message=MessageContent(content='AI plays a significant role in news production, from automating news writing to aiding in fact-checking and content curation, impacting the efficiency, accessibility, and potentially the objectivity of the news.', role='assistant'), logprobs=None)] created=1747397316 id='' model='microsoft/Phi-3-mini-4k-instruct' object='chat.completion' usage={'completion_tokens': 46, 'prompt_tokens': 24, 'total_tokens': 70}\n",
      "Content: AI plays a significant role in news production, from automating news writing to aiding in fact-checking and content curation, impacting the efficiency, accessibility, and potentially the objectivity of the news.\n",
      "Model used: microsoft/Phi-3-mini-4k-instruct\n",
      "Tokens used: 70\n"
     ]
    }
   ],
   "source": [
    "print(\"Full response object:\", response)\n",
    "print(\"Content:\", response.get_content())\n",
    "print(\"Model used:\", response.model)\n",
    "print(\"Tokens used:\", response.usage.get(\"total_tokens\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cUCjMbySGBG"
   },
   "source": [
    "#### Exercise 1 (b): Asking for current information\n",
    "\n",
    "Now let's try tweaking the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uzpf1-YuSI1h"
   },
   "outputs": [],
   "source": [
    "response = llm.generate(\n",
    "    messages=[UserMessage(\"Get me the news today.\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQ1aJ_E5UWI1",
    "outputId": "05b71c53-4182-4557-af1f-5bbe569a9476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I can't access real-time information or browse the internet. I'm designed to work with a wide array of information up until my last update in early 2023. For the latest news, I recommend checking a reliable news source like BBC News, CNN, The New York Times, or directly accessing news apps or websites according to your preference.\n"
     ]
    }
   ],
   "source": [
    "print(response.get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msnDXYROS7fc"
   },
   "source": [
    "**Expected output:** The LLM will not be able to fetch the latest news. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tBHfDaKTDQy"
   },
   "source": [
    "#### Exercise 1 (c): Prompt engineering\n",
    "\n",
    "Try different prompts to understand prompt design:\n",
    "\n",
    "* Request a list\n",
    "* Return a single word\n",
    "* Ask for a creative story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50bTbM4hS4vk"
   },
   "outputs": [],
   "source": [
    "response = llm.generate(\n",
    "    # TODO: insert your code here\n",
    "    messages=[UserMessage(\"\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4iq4uAkUdLK"
   },
   "outputs": [],
   "source": [
    "print(response.get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yo-uLJ0GSK8N"
   },
   "source": [
    "#### [Optional] Exercise 1 (d): Parameter tweaking\n",
    "\n",
    "You can also try experimenting with model parameters, for example:\n",
    "\n",
    "* set the temperature (A float value to control the temperature of the model. Used to optimize for consistency and creativity)\n",
    "* Optional: Re-initialze the client with a different model: https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xvO5dsMTGeR"
   },
   "outputs": [],
   "source": [
    "response = llm.generate(\n",
    "    messages=[UserMessage(\"What is the impact of AI on the news?\")]\n",
    "    # TODO: insert your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WFXtP4pTLiZ"
   },
   "source": [
    "**Discussion**: What makes a good prompt? How do parameters affect output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a9nbyvpPuvG"
   },
   "source": [
    "#### [Optional] Exercise 1 (e): Chat completion using a prompty file for context\n",
    "\n",
    "In Dapr Agents, we can use prompty to provide additional context/history to our LLM. Before running the cells below, nalyze its contents of `prompt_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GOKWlHiuYPx8"
   },
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"\n",
    "---\n",
    "name: Basic Prompt\n",
    "description: A basic prompt that uses the chat API to answer questions\n",
    "model:\n",
    "    api: chat\n",
    "    configuration:\n",
    "        type: huggingface\n",
    "        name: microsoft/Phi-3-mini-4k-instruct\n",
    "    parameters:\n",
    "        max_tokens: 128\n",
    "        temperature: 0.2\n",
    "inputs:\n",
    "  question:\n",
    "    type: string\n",
    "sample:\n",
    "  \"question\": \"Who is the most famous person in the world?\"\n",
    "---\n",
    "system:\n",
    "You are an AI assistant who helps people find information.\n",
    "As the assistant, you answer questions briefly, succinctly.\n",
    "\n",
    "user:\n",
    "{{question}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Wfb6nFMQZjJ",
    "outputId": "9271470c-8ef9-46ec-8d76-52a81643565f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choices=[Choice(finish_reason='length', index=0, message=MessageContent(content=\"I'm Phi and my purpose as Microsoft language model GPT-3 developed by MS research team led to assist users in various tasks involving natural languages processing like text generation or translation among others while maintaining user privacy at all times without personal data storage capabilities beyond this interaction session for security reasons according with our policy guidelines on ethical use of technology which includes respectful communication practices that promote positive interactions between humans across diverse backgrounds irrespective their race gender age religion sexual orientation nationality etc., ensuring equal opportunities accessibility inclusivity diversified representation equitable distribution benefits fairness transparency accountability integrity hon\", role='assistant'), logprobs=None)] created=1747397342 id='' model='microsoft/Phi-3-mini-4k-instruct' object='chat.completion' usage={'completion_tokens': 128, 'prompt_tokens': 36, 'total_tokens': 164}\n"
     ]
    }
   ],
   "source": [
    "llm = HFHubChatClient.from_prompty(prompt_text)\n",
    "print(llm.generate(input_data={\"question\":\"What is your name?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft_ztIwTZb0x"
   },
   "source": [
    "Change the system instructions to make the assistant more formal, humorous, or detailed. Then re-execute the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Troubleshooting\n",
    "\n",
    "1. **API Key Issues**: If you see an authentication error, verify your HuggingFace API key in the `helper_secrets.py` file\n",
    "2. **Python Version**: If you encounter compatibility issues, make sure you're using Python 3.10+\n",
    "3. **Environment Activation**: Ensure your virtual environment is activated before running examples\n",
    "4. **Import Errors**: If you see module not found errors, verify that `pip install -r requirements.txt` completed successfully"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
